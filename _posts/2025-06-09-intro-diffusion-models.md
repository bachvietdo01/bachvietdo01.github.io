---
layout: post
title: "A Gentle Intro to Diffusion Models"
author: "Bach Do"
categories: journal
tags: [documentation,sample]
image: a2_diffusion_model_logo.gif
---

Diffusion models have recently emerged as a powerful class of deep generative models, achieving state-of-the-art results in image generation, audio synthesis, protein design, and many scientific domains. Notable works in the literature include [Sohl-Dickstein et al. (2015)](https://arxiv.org/abs/1503.03585), [Song, Ermon et al. (2019)](https://arxiv.org/abs/1907.05600), [Ho et al. (2020)](https://arxiv.org/abs/2006.11239), [Song, Sohl-Dickstein et al. (2020)](https://arxiv.org/abs/2011.13456), and [Karras et al. (2022)](https://arxiv.org/abs/2206.00364). 

This introduction explains the core idea of diffusion models as stochastic flows generated by a Stochastic Differential Equation (SDE) and walks through a full example of how to sample an 11-component Gaussian mixture data starting from a standard normal initial distribution. The full codes can be found [here](https://github.com/bachvietdo01/generative_models/tree/main/diffusion_model).


## Stochastic Flows by SDE

Like Flow Matching models, Diffusion models learn a probability path from an initial "noise" distribution to a target data distribution. A common view of diffusion models is learning to reverse a forward process that progressively diffuse data into noise. However, they can also be seen as a stochastic counterpart to flow matching models (see Holderrieth et al., 2025). 

[Flow matching](https://bachvietdo01.github.io/intro-to-flow-matching-models) constructs a probability path $\\{ X_t \\}$ with time index $t \in [0,1]$ where $X_0 \sim p_{\text{init}}(\cdot)$ and $X_1 \sim p_{\text{data}}(\cdot)$ where the flow $\\{ X_t \\}$ is the solution to the following ODE,

$$
\begin{align}
\cfrac{dX_t}{dt} = u_t(X_t).
\end{align}
$$

In contrast, diffusion models are stochastic flows, the solutions to the Stochastic Differential Equation:

$$
\begin{align}
dX_t = u_t(X_t)dt + \frac{\sigma_t^2}{2} \cdot \nabla \log p_t(X_t) dt + \sigma_t d W_t \quad (1)
\end{align}
$$

Here, $X_t \sim p_t(\cdot)$ traces the marginal probability path, $u_t(X_t)$ is the deterministic vector field, $\sigma_t^2$ is the diffusion coefficient, and $W_t$ denotes Brownian motion. Personally, I find the SDE perspective more intuitive and rigorous.

<p align="center">
<img src="https://github.com/bachvietdo01/bachvietdo01.github.io/blob/main/assets/img/a2_ode_traj.png?raw=true" alt="ode_traj" width="450"/>
<img src="https://github.com/bachvietdo01/bachvietdo01.github.io/blob/main/assets/img/a2_sde_traj.png?raw=true" alt="sde_traj" width="450"/> 
</p>

The SDE $(1)$ implies that for sufficiently small $h > 0$, 

$$
\begin{aligned}
X_{t+h} = X_t + h\cdot u_t + h \cdot \frac{\sigma_t^2}{2} \nabla \log p_t(X_t) + \sqrt{h} \cdot \sigma_t \cdot N(\cdot \mid 0, I).
\end{aligned}
$$

The last term, $\sqrt{h} \cdot N(\cdot \mid 0 \\; , \\; I)$, is the outcome of taking a small changes of Brownian Motion $dW_t$, also known as white noise—the formal derivative of Brownian motion.


## Construct Conditional and Marginal Probability Path

<p align="center">
<img src="https://github.com/bachvietdo01/bachvietdo01.github.io/blob/main/assets/img/a1_gcp.png?raw=true" alt="a1_gcp" width="380"/>
<br>
<em>Gaussian Conditionn path converges to data point z as time t goes to 1</em>
</p>

Similar to Flow Matching, given data sample $z \sim p_{\text{data}}$, we first define a conditional probability path $p_0(\cdot \mid z), \ldots, p_t(\cdot \mid z), \ldots, p_1(\cdot \mid z)$ tending to the point mass $\delta_z(\cdot)$. A common choice is a conditional Gaussian: $p_t(\cdot \mid z) := N(\cdot \mid \alpha_t z \\;, \\; \beta_t^2 I)$, where the noise schedulers $\alpha_t \to 1$ and $\beta_t^2 \to 0$ as $t \to 1$.

As shown in the Flow Matching introduction, this defines a deterministic flow solving the ODE $\frac{dX_t}{dt} = u_t(X_t \mid z)$ with
$u_t(x \mid z) = \left(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t \right) z + \frac{\dot{\beta}_t}{\beta_t} x$.

Using Bayes’ rule, the marginal distribution is $p_t(x) = \int p_t(x \mid z) p_{\text{data}}(z), dz$. It can be shown that $u_t$ and $\nabla \log p_t$ satisfy the **Fokker–Planck equation**, and by Theorem 15 of Holderrieth et al. (2025), if $X_t$ solves the SDE $(1)$, then $X_t$ follows a marginal probability path with $X_1 \sim p_{\text{data}}$.

## Conditional Score Matching Loss Objective

Under Gaussian Probability path, $\nabla \log p_t(x \mid z) = - \frac{x - \alpha_t z}{\beta_t^2}$. This fact can be used to show that,

$$
\begin{align}
u_t(x) = \left( \beta_t^2 \cfrac{\dot \alpha_t}{\alpha_t} - \dot \beta_t \beta_t  \right) \nabla \log p_t(x) + \cfrac{\dot \alpha_t}{\alpha_t}x
\end{align}
$$

As such, the SDE $(1)$ can be rewritten with only score function,

$$
\begin{align}
dX_t = \left( \beta_t^2 \cfrac{\dot \alpha_t}{\alpha_t} - \dot \beta_t \beta_t  + \frac{\sigma_t^2}{2} \right) \nabla \log p_t(X_t) dt + \sigma_t d W_t 
\end{align}
$$

This form of (1) suggests that to simulate the SDE, we just need to learn the score function $\nabla \log p_t(x)$ using a neural network. This is the core idea behind Score Matching Diffusion Models. The natural loss objective is the Score Matching loss with $s^{\theta}_t$ a neural net with parameter $\theta$:

$$
\begin{align}
L_{\text{SM}}(\theta) = \mathbb{E} \lVert s^{\theta}_t(x) - \log p_t(x)\rVert^2 
\end{align}
$$

Unfortunately, since the form of the density $p_{\text{data}}(z)$ is generally unknown, $\log p_t(x)$ is intractable. However, similar to Flow Matching, it turns out $L_{\text{SM}}(\theta) = L_{\text{CSM}}(\theta) + C$ where $C$ is a constant independent of $\theta$. The Conditional Score Matching loss is given by:

$$
\begin{align}
L_{\text{CSM}}(\theta) =  \mathbb{E} \lVert s^{\theta}_t(x) - \log p_t(x \mid z)\rVert^2 
\end{align}
$$

## Putting it all together into practice

Alright, that's enough foundation and let's dive into the codes.

#### Step 0: define $p_{\text{init}}$ as Standard Normal distribution and the target distribution is a 11-component mixture of normals.

```
from gaussian import Sampleable

class StandardNormal(nn.Module, Sampleable):
    """
    Sampleable wrapper around torch.randn
    """
    def __init__(self, shape: List[int], std: float = 1.0):
        """
        shape: shape of sampled data
        """
        super().__init__()
        self.shape = shape
        self.std = std
        self.dummy = nn.Buffer(torch.zeros(1)) # Will automatically be moved when self.to(...) is called...

    def sample(self, num_samples) -> torch.Tensor:
        return self.std * torch.randn(num_samples, *self.shape).to(self.dummy.device)
```

```
from gaussian import Gaussian, GaussianMixture
from ultility import plot_comparison_heatmap

# Constants for the duration of our use of Gaussian conditional probability paths, to avoid polluting the namespace...
PARAMS = {
    "scale": 15.0,
    "target_scale": 10.0,
    "target_std": 1.0,
}

p_init = Gaussian.standard(dim=2, std = 1.0).to(device)
p_data = GaussianMixture.symmetric_2D(nmodes=11, std=PARAMS["target_std"], scale=PARAMS["target_scale"]).to(device)
plot_comparison_heatmap(p_init, p_data, PARAMS['scale'])
```

<p align="center">
<img src="https://github.com/bachvietdo01/bachvietdo01.github.io/blob/main/assets/img/a1_target_and_initial_dist.png?raw=true" alt="vectorfieldflow" width="1000"/>
</p>


#### Step 1: builds Gaussian Conditional path with noise scheduler $\alpha_t = t$ and $\beta_t = \sqrt{1-t}$

```
class LinearAlpha:
    """
    Implements alpha_t = t
    """

    def __init__(self):
        # Check alpha_t(0) = 0
        assert torch.allclose(
            self(torch.zeros(1,1,1,1)), torch.zeros(1,1,1,1)
        )
        # Check alpha_1 = 1
        assert torch.allclose(
            self(torch.ones(1,1,1,1)), torch.ones(1,1,1,1)
        )

    def __call__(self, t: torch.Tensor) -> torch.Tensor:
        """
        Args:
            - t: time (num_samples, 1)
        Returns:
            - alpha_t (num_samples, 1)
        """
        return t

    def dt(self, t: torch.Tensor) -> torch.Tensor:
        """
        Evaluates d/dt alpha_t.
        Args:
            - t: time (num_samples, 1)
        Returns:
            - d/dt alpha_t (num_samples, 1)
        """
        return torch.ones_like(t)

class SquareRootBeta:
    """
    Implements beta_t = rt(1-t)
    """
    def __init__(self):
        # Check beta_0 = 1
        assert torch.allclose(
            self(torch.zeros(1,1,1,1)), torch.ones(1,1,1,1)
        )
        # Check beta_1 = 0
        assert torch.allclose(
            self(torch.ones(1,1,1,1)), torch.zeros(1,1,1,1)
        )

    def __call__(self, t: torch.Tensor) -> torch.Tensor:
        """
        Args:
            - t: time (num_samples, 1)
        Returns:
            - beta_t (num_samples, 1)
        """
        return torch.sqrt(1 - t)

    def dt(self, t: torch.Tensor) -> torch.Tensor:
        """
        Evaluates d/dt alpha_t.
        Args:
            - t: time (num_samples, 1)
        Returns:
            - d/dt alpha_t (num_samples, 1)
        """
        return - 0.5 / (torch.sqrt(1 - t) + 1e-4)
```

```
class GaussianConditionalProbabilityPath(nn.Module):
    def __init__(self, p_data: Sampleable, alpha: LinearAlpha, beta: SquareRootBeta):
        super().__init__()
        p_init = StandardNormal(shape = [p_data.dim], std = 1.0)
        self.p_init = p_init
        self.p_data = p_data
        
        self.alpha = alpha
        self.beta = beta

    def sample_marginal_path(self, t: torch.Tensor) -> torch.Tensor:
        """
        Samples from the marginal distribution p_t(x) = p_t(x|z) p(z)
        Args:
            - t: time (num_samples, 1, 1, 1)
        Returns:
            - x: samples from p_t(x), (num_samples, c, h, w)
        """
        num_samples = t.shape[0]
        # Sample conditioning variable z ~ p(z)
        z, _ = self.sample_conditioning_variable(num_samples) # (num_samples, c, h, w)
        # Sample conditional probability path x ~ p_t(x|z)
        x = self.sample_conditional_path(z, t) # (num_samples, c, h, w)
        return x

    def sample_conditioning_variable(self, num_samples: int) -> torch.Tensor:
        """
        Samples the conditioning variable z and label y
        Args:
            - num_samples: the number of samples
        Returns:
            - z: (num_samples, c, h, w)
            - y: (num_samples, label_dim)
        """
        return self.p_data.sample(num_samples)

    def sample_conditional_path(self, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
        """
        Samples from the conditional distribution p_t(x|z)
        Args:
            - z: conditioning variable (num_samples, c, h, w)
            - t: time (num_samples, 1, 1, 1)
        Returns:
            - x: samples from p_t(x|z), (num_samples, c, h, w)
        """
        return self.alpha(t) * z + self.beta(t) * torch.randn_like(z)

    def conditional_vector_field(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
        """
        Evaluates the conditional vector field u_t(x|z)
        Args:
            - x: position variable (num_samples, c, h, w)
            - z: conditioning variable (num_samples, c, h, w)
            - t: time (num_samples, 1, 1, 1)
        Returns:
            - conditional_vector_field: conditional vector field (num_samples, c, h, w)
        """
        alpha_t = self.alpha(t) # (num_samples, 1, 1, 1)
        beta_t = self.beta(t) # (num_samples, 1, 1, 1)
        dt_alpha_t = self.alpha.dt(t) # (num_samples, 1, 1, 1)
        dt_beta_t = self.beta.dt(t) # (num_samples, 1, 1, 1)

        return (dt_alpha_t - dt_beta_t / beta_t * alpha_t) * z + dt_beta_t / beta_t * x

    def conditional_score(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
        """
        Evaluates the conditional score of p_t(x|z)
        Args:
            - x: position variable (num_samples, c, h, w)
            - z: conditioning variable (num_samples, c, h, w)
            - t: time (num_samples, 1, 1, 1)
        Returns:
            - conditional_score: conditional score (num_samples, c, h, w)
        """
        alpha_t = self.alpha(t)
        beta_t = self.beta(t)
        return (z * alpha_t - x) / beta_t ** 2
```

#### Step 3: Learn vector field $u_t(x)$ and score $\nabla \log p_t(x)$ with MLP(s)

```
class MLPVectorField(torch.nn.Module):
    """
    MLP-parameterization of the learned vector field u_t^theta(x)
    """
    def get_mlp(self, dims: List[int], activation: Type[torch.nn.Module] = torch.nn.SiLU):
        mlp = []
        for idx in range(len(dims) - 1):
            mlp.append(torch.nn.Linear(dims[idx], dims[idx + 1]))
            if idx < len(dims) - 2:
                mlp.append(activation())
        return torch.nn.Sequential(*mlp)

    def __init__(self, dim: int, hiddens: List[int]):
        super().__init__()
        self.dim = dim
        self.net = self.get_mlp([dim + 1] + hiddens + [dim])

    def forward(self, x: torch.Tensor, t: torch.Tensor):
        """
        Args:
        - x: (bs, dim)
        Returns:
        - u_t^theta(x): (bs, dim)
        """
        xt = torch.cat([x,t], dim=-1)
        return self.net(xt)
```

```
from trainer import Trainer

class ConditionalFlowMatchingTrainer(Trainer):
    def __init__(self, path: GaussianConditionalProbabilityPath, model: MLPVectorField, **kwargs):
        super().__init__(model, **kwargs)
        self.path = path

    def get_train_loss(self, batch_size: int) -> torch.Tensor:
      z = self.path.p_data.sample(batch_size)
      t = torch.rand(batch_size, 1)
      x = self.path.sample_conditional_path(z, t)
      u_theta = self.model(x, t)
      u_ref = self.path.conditional_vector_field(x, z, t)

      return torch.mean((u_theta - u_ref)**2)


# Construct learnable vector field
flow_model = MLPVectorField(dim=2, hiddens=[1024,16])

# Construct trainer
trainer = ConditionalFlowMatchingTrainer(path, flow_model)
losses = trainer.train(num_epochs=5000, device=device, lr=1e-3, batch_size=1000)
```

```
class MLPScore(torch.nn.Module):
    """
    MLP-parameterization of the learned score field
    """
    def __get_mlp(self, dims: List[int], activation: Type[torch.nn.Module] = torch.nn.SiLU):
        mlp = []
        for idx in range(len(dims) - 1):
            mlp.append(torch.nn.Linear(dims[idx], dims[idx + 1]))
            if idx < len(dims) - 2:
                mlp.append(activation())
        return torch.nn.Sequential(*mlp)


    def __init__(self, dim: int, hiddens: List[int]):
        super().__init__()
        self.dim = dim
        self.net = self.__get_mlp([dim + 1] + hiddens + [dim])

    def forward(self, x: torch.Tensor, t: torch.Tensor):
        """
        Args:
        - x: (bs, dim)
        Returns:
        - s_t^theta(x): (bs, dim)
        """
        xt = torch.cat([x,t], dim=-1)
        return self.net(xt)

from trainer import Trainer

class ConditionalScoreMatchingTrainer(Trainer):
    def __init__(self, path: GaussianConditionalProbabilityPath, model: MLPScore, **kwargs):
        super().__init__(model, **kwargs)
        self.path = path

    def get_train_loss(self, batch_size: int) -> torch.Tensor:
        z = self.path.p_data.sample(batch_size)
        t = torch.rand(batch_size, 1)
        x = self.path.sample_conditional_path(z, t)

        score_theta = self.model(x, t)
        score_ref = self.path.conditional_score(x, z, t)

        return torch.mean((score_theta - score_ref)**2)

# Construct learnable vector field
score_model = MLPScore(dim=2, hiddens=[1024,16])

# Construct trainer
trainer = ConditionalScoreMatchingTrainer(path, score_model)
losses = trainer.train(num_epochs=5000, device=device, lr=1e-3, batch_size=1000)
```

#### Step 4: Generate sample by simulating SDE with learned vector field and score function

```
from sde import LangevinFlowSDE, EulerMaruyamaSimulator

num_samples = 1000
num_timesteps = 300
num_marginals = 3

sigma = 0.5

sde = LangevinFlowSDE(flow_model, score_model, sigma)
simulator = EulerMaruyamaSimulator(sde)
x0 = path.p_init.sample(num_samples) # (num_samples, 2)
ts = torch.linspace(0.0, 1.0, num_timesteps).view(1,-1,1).expand(num_samples,-1,1).to(device) # (num_samples, nts, 1)
xts = simulator.simulate_with_trajectory(x0, ts) # (bs, nts, dim)
```

```
from importlib import reload
from ultility import plot_generated_sample

plot_generated_sample(xts, ts, p_init, p_data, scale = PARAMS['scale'], num_timesteps=num_timesteps, num_marginals=num_marginals)
```

<p align="center">
<img src="https://github.com/bachvietdo01/bachvietdo01.github.io/blob/main/assets/img/a2_sampled_gm.png?raw=true" alt="vectorfieldflow" width="1000"/>
</p>


## Reference

[1] Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., & Le, M. (2022). Flow matching for generative modeling. arXiv preprint arXiv:2210.02747.

[2] Holderrieth, Peter, and Ezra Erives. "An Introduction to Flow Matching and Diffusion Models." arXiv preprint arXiv:2506.02070 (2025).

[3] Sohl-Dickstein, Jascha, et al. "Deep unsupervised learning using nonequilibrium thermodynamics." International conference on machine learning. pmlr, 2015.

[4] Song, Yang, and Stefano Ermon. "Generative modeling by estimating gradients of the data distribution." Advances in neural information processing systems 32 (2019).

[5] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models." Advances in neural information processing systems 33 (2020): 6840-6851.

[6] Song, Yang, et al. "Score-based generative modeling through stochastic differential equations." arXiv preprint arXiv:2011.13456 (2020).

[7] Karras, Tero, et al. "Elucidating the design space of diffusion-based generative models." Advances in neural information processing systems 35 (2022): 26565-26577.






